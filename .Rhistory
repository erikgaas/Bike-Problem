direct.label.ggplot(plot,col=black)
direct.label.ggplot(plot)
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country),group=Countries)+
geom_point()+
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6)
plot
direct.label.ggplot(plot)
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country),group=Countries,col="black")+
geom_point()+
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6)
my.lm <-lm(Happiness~log.2000, data=data_for_R)
summary(my.lm)
my.lm
direct.label.ggplot(plot)
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country),group=Countries,colour="black")+
geom_point()+
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6)
my.lm <-lm(Happiness~log.2000, data=data_for_R)
summary(my.lm)
my.lm
direct.label.ggplot(plot)
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country),group=Countries)+
geom_point(colour="black")+
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6)
direct.label.ggplot(plot)
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country),group=Countries)+
geom_point(colour="black")+
geom_smooth(method=lm,colour="black") +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6,colour="black")
direct.label.ggplot(plot)
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country),group=Countries)+
geom_point(colour="black")+
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6)+
scale_colour_manual(values = rep("black", length(unique(Country))))
direct.label.ggplot(plot)
plot
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country,col="black"),group=Countries)+
geom_point(colour="black")+
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country,col="black"),hjust=0,vjust=0, size = 6)
plot
direct.label.ggplot(plot)
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country,col="black"),group=Countries)+
geom_point(colour="black")+
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6)
plot
direct.label.ggplot(plot)
direct.label(plot)
direct.label(plot)
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country))+
geom_point()+
scale_colour_manual
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6)
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country))+
geom_point()+
scale_colour_manual
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6)
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country))+
geom_point()+
scale_colour_manual
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6)
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country))+
geom_point()+
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6)
plot
summary(my.lm)
plot<-ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country))+
geom_point()+
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 5)
ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country))+
geom_point()+
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 5)
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6)
ggplot(data=data_for_R,aes(Happiness,log.2000,label=Country))+
geom_point()+
geom_smooth(method=lm) +
ggtitle("Log GDP vs Happiness Metric Top 100 Countries")+
geom_text(aes(label=Country),hjust=0,vjust=0, size = 6)
source('~/Desktop/Bike-Problem/modeling_rforest_win_jkg.R', echo=TRUE)
casual_forest_bias
test <- read.delim("~/Desktop/Bike-Problem/filtered_test.txt")
test$hour <- factor(test$hour)
test$year <- factor(test$year)
test$season <- factor(test$season)
test$holiday <- factor(test$holiday)
test$workingday <- factor(test$workingday)
test$weather <- factor(test$weather)
train <- read.delim("~/Desktop/Bike-Problem/filtered_train.txt")
train$hour <- factor(train$hour)
train$year <- factor(train$year)
train$season <- factor(train$season)
train$holiday <- factor(train$holiday)
train$workingday <- factor(train$workingday)
train$weather <- factor(train$weather)
library(gbm)
library(caret)
library(doParallel)
library(randomForest)
#hold out .25 to estimate kaggle score
train_idx <-createDataPartition(train$casual,p=.75,list=F)
train_train <-train[train_idx,]
train_train$casual <-log(train_train$casual+1)
train_train$registered <-log(train_train$registered+1)
#trying out logs
train_test <- train[-train_idx,]
train_test_casual <- train_test$casual
train_test_registered <- train_test$registered
train_test <- train_test[,-c(13,14)]
casual_formula <-as.formula(paste0("casual~",paste(colnames(train[-c(13,14)]),collapse="+")))
registered_formula <- as.formula(paste0("registered~",paste(colnames(train[-c(13,14)]),collapse="+")))
##well first try didn't work so well. so let's train some parameters
library(caret)
##massively shrunk parameters let's see how this runs
library(gbm)
##lets try to average em
gbm_casual<- gbm(casual_formula,n.trees=3000,data=train_train,
distribution="gaussian",interaction.depth=10,
train.fraction=.8,cv.folds=10)
gbm_registered <- gbm(registered_formula,n.trees=5000,data=train_train,
distribution="gaussian",interaction.depth=10,
train.fraction=.8,cv.folds=10)
##basic random forest
casual_forest_bias <- randomForest(casual_formula,data=train_train,ntree=1500,mtry=5,importance=T,
corr.bias=T)
regist_forest_bias<- randomForest(registered_formula,data=train_train,ntree=1500,mtry=5,importance=T,
corr.bias=T)
##predict kaggle score
predicted_casual_f <- predict(casual_forest_bias,train_test)
predicted_registered_f <- predict(regist_forest_bias,train_test)
predicted_total_f <- predicted_casual_f + predicted_registered_f
predicted_casual_g <- predict(gbm_casual,train_test)
predicted_registered_g <- predict(gbm_registered,train_test)
predicted_total_g <- predicted_casual_g + predicted_registered_g
predicted_total <- round((predicted_total_g+predicted_total_f)/2,0)
actual_total <- train_test_casual + train_test_registered
rmsle <- ((1/length(train_test_registered))*sum((log(predicted_total+1)-log(actual_total+1))**2))**.5
predicted_total <- round((predicted_total_g+predicted_total_f)/2,0)
View(predicted_total)
predicted_total_f
predicted_total_f
V(predicted_total_f)
View(predicted_total_f)
##predict kaggle score
predicted_casual_f <- predict(casual_forest_bias,train_test)
predicted_registered_f <- predict(regist_forest_bias,train_test)
predicted_total_f <- predicted_casual_f + predicted_registered_f
predicted_casual_g <- predict(gbm_casual,train_test)
predicted_registered_g <- predict(gbm_registered,train_test)
predicted_total_g <- predicted_casual_g + predicted_registered_g
predicted_total <- (predicted_total_g+predicted_total_f)/2
predicted_total <- round(exp(predicted_total),0)
actual_total <- train_test_casual + train_test_registered
rmsle <- ((1/length(train_test_registered))*sum((log(predicted_total+1)-log(actual_total+1))**2))**.5
##predict kaggle score
predicted_casual_f <- predict(casual_forest_bias,train_test)
predicted_registered_f <- predict(regist_forest_bias,train_test)
predicted_total_f <- predicted_casual_f + predicted_registered_f
predicted_casual_g <- predict(gbm_casual,train_test)
predicted_registered_g <- predict(gbm_registered,train_test)
predicted_total_g <- predicted_casual_g + predicted_registered_g
predicted_total <- (predicted_total_g+predicted_total_f)/2
predicted_total <- (predicted_total_g+predicted_total_f)*.5
predicted_casual_f <- predict(casual_forest_bias,train_test)
predicted_registered_f <- predict(regist_forest_bias,train_test)
predicted_total_f <- predicted_casual_f + predicted_registered_f
predicted_casual_g <- predict(gbm_casual,train_test)
predicted_registered_g <- predict(gbm_registered,train_test)
predicted_total_g <- predicted_casual_g + predicted_registered_g
predicted_total <- (predicted_total_g+predicted_total_f)*.5
##predict kaggle score
predicted_casual_f <- exp(predict(casual_forest_bias,train_test))
predicted_registered_f <- exp(predict(regist_forest_bias,train_test))
predicted_total_f <- predicted_casual_f + predicted_registered_f
predicted_casual_g <- exp(predict(gbm_casual,train_test))
predicted_registered_g <- exp(predict(gbm_registered,train_test))
predicted_total_g <- predicted_casual_g + predicted_registered_g
predicted_total <- (predicted_total_g+predicted_total_f)*.5
actual_total <- train_test_casual + train_test_registered
rmsle <- ((1/length(train_test_registered))*sum((log(predicted_total+1)-log(actual_total+1))**2))**.5
predicted_total <- predicted_total_g
rmsle <- ((1/length(train_test_registered))*sum((log(predicted_total+1)-log(actual_total+1))**2))**.5
predicted_total<-predicted_total_f
rmsle <- ((1/length(train_test_registered))*sum((log(predicted_total+1)-log(actual_total+1))**2))**.5
predicted_total <- round((predicted_total_g+predicted_total_f)*.5,0)
rmsle <- ((1/length(train_test_registered))*sum((log(predicted_total+1)-log(actual_total+1))**2))**.5
source('~/Desktop/Bike-Problem/modeling_mac_jkg.R', echo=TRUE)
View(predicted_total)
View(predicted_total)
control<-trainControl(method="LGOCV")
enetGrid <-expand.grid(.lambda = c(.001,.01,.1),.fraction=seq(.05,1,length=10))
control<-trainControl(method="LGOCV")
enetGrid <-expand.grid(.lambda = c(.001,.01,.1),.fraction=seq(.05,1,length=10))
enet_casual <-train(casual_formula,
data=train_train,
method = "enet",
tuneGrid=enetGrid,
trControl= control,
preProc = c("center","scale"))
enetTune_registered <-train(casual_formula,
data=train_train,
method = "enet",
tuneGrid=enetGrid,
trControl= control,
preProc = c("center","scale"))
test <- read.delim("~/Desktop/Bike-Problem/filtered_test.txt")
test$hour <- factor(test$hour)
test$year <- factor(test$year)
test$season <- factor(test$season)
test$holiday <- factor(test$holiday)
test$workingday <- factor(test$workingday)
test$weather <- factor(test$weather)
train <- read.delim("~/Desktop/Bike-Problem/filtered_train.txt")
train$hour <- factor(train$hour)
train$year <- factor(train$year)
train$season <- factor(train$season)
train$holiday <- factor(train$holiday)
train$workingday <- factor(train$workingday)
train$weather <- factor(train$weather)
library(gbm)
library(caret)
library(doParallel)
library(randomForest)
# train$casual <-log(train$casual+1)
# train$registered <-log(train$registered+1)
#hold out .25 to estimate kaggle score
train_idx <-createDataPartition(train$casual,p=.75,list=F)
train_train <-train[train_idx,]
train_train$casual <-log(train_train$casual+1)
train_train$registered <-log(train_train$registered+1)
#trying out logs
train_test <- train[-train_idx,]
train_test_casual <- train_test$casual
train_test_registered <- train_test$registered
train_test <- train_test[,-c(13,14)]
casual_formula <-as.formula(paste0("casual~",paste(colnames(train[-c(13,14)]),collapse="+")))
registered_formula <- as.formula(paste0("registered~",paste(colnames(train[-c(13,14)]),collapse="+")))
##well first try didn't work so well. so let's train some parameters
library(caret)
##massively shrunk parameters let's see how this runs
library(gbm)
control<-trainControl(method="LGOCV")
enetGrid <-expand.grid(.lambda = c(.001,.01,.1),.fraction=seq(.05,1,length=10))
enet_casual <-train(casual_formula,
data=train_train,
method = "enet",
tuneGrid=enetGrid,
trControl= control,
preProc = c("center","scale"))
test <- read.delim("~/Desktop/Bike-Problem/filtered_test.txt")
test$hour <- factor(test$hour)
test$year <- factor(test$year)
test$season <- factor(test$season)
test$holiday <- factor(test$holiday)
test$workingday <- factor(test$workingday)
test$weather <- factor(test$weather)
train <- read.delim("~/Desktop/Bike-Problem/filtered_train.txt")
train$hour <- factor(train$hour)
train$year <- factor(train$year)
train$season <- factor(train$season)
train$holiday <- factor(train$holiday)
train$workingday <- factor(train$workingday)
train$weather <- factor(train$weather)
library(gbm)
library(caret)
library(doParallel)
library(randomForest)
# train$casual <-log(train$casual+1)
# train$registered <-log(train$registered+1)
#hold out .25 to estimate kaggle score
train_idx <-createDataPartition(train$casual,p=.75,list=F)
train_train <-train[train_idx,]
train_train$casual <-log(train_train$casual+1)
train_train$registered <-log(train_train$registered+1)
#trying out logs
train_test <- train[-train_idx,]
train_test_casual <- train_test$casual
train_test_registered <- train_test$registered
train_test <- train_test[,-c(13,14)]
casual_formula <-as.formula(paste0("casual~",paste(colnames(train[-c(13,14)]),collapse="+")))
registered_formula <- as.formula(paste0("registered~",paste(colnames(train[-c(13,14)]),collapse="+")))
##well first try didn't work so well. so let's train some parameters
library(caret)
##massively shrunk parameters let's see how this runs
library(gbm)
cl <-makeCluster(4)
registerDoParallel(cl)
control<-trainControl(method="LGOCV")
enetGrid <-expand.grid(.lambda = c(.001,.01,.1),.fraction=seq(.05,1,length=10))
enet_casual <-train(casual_formula,
data=train_train,
method = "enet",
tuneGrid=enetGrid,
trControl= control,
preProc = c("center","scale"))
enetTune_registered <-train(casual_formula,
data=train_train,
method = "enet",
tuneGrid=enetGrid,
trControl= control,
preProc = c("center","scale"))
source('~/Desktop/Bike-Problem/enet_attempt.R', echo=TRUE)
enetGrid<-expand.grid(.lambda = c(.001,.01,.1),.fraction = seq(.05,1,length=10))
control <- trainControl(method="LGOCV")
enetGrid<-expand.grid(.lambda = c(.001,.01,.1),.fraction = seq(.05,1,length=10))
enet_casual <- train(as.matrix(train_train[,-13]),as.matrix(train_train[,13]),
method = "enet",
tuneGrid=enetGrid,
trControl = control,
preProc =c("center","scale"))
test <- read.delim("~/Desktop/Bike-Problem/filtered_test.txt")
test$hour <- factor(test$hour)
test$year <- factor(test$year)
test$season <- factor(test$season)
test$holiday <- factor(test$holiday)
test$workingday <- factor(test$workingday)
test$weather <- factor(test$weather)
train <- read.delim("~/Desktop/Bike-Problem/filtered_train.txt")
train$hour <- factor(train$hour)
train$year <- factor(train$year)
train$season <- factor(train$season)
train$holiday <- factor(train$holiday)
train$workingday <- factor(train$workingday)
train$weather <- factor(train$weather)
library(gbm)
library(caret)
library(doParallel)
library(randomForest)
cl <-makeCluster(4)
registerDoParallel(cl)
#hold out .25 to estimate kaggle score
train_idx <-createDataPartition(train$casual,p=.75,list=F)
train_train <-train[train_idx,]
train_train$casual <-log(train_train$casual+1)
train_train$registered <-log(train_train$registered+1)
#trying out logs
train_test <- train[-train_idx,]
train_test_casual <- train_test$casual
train_test_registered <- train_test$registered
train_test <- train_test[,-c(13,14)]
control <- trainControl(method="LGOCV")
enetGrid<-expand.grid(.lambda = c(.001,.01,.1),.fraction = seq(.05,1,length=10))
enet_casual <- train(as.matrix(train_train[,-c(13,14)]),as.matrix(train_train[,13]),
method = "enet",
tuneGrid=enetGrid,
trControl = control,
preProc =c("center","scale"))
enet_registered <- train(as.matrix(train_train[,-c(13,14)]),as.matrix(train_train[,14]),
method = "enet",
tuneGrid=enetGrid,
trControl=control,
preProc =c("center","scale"))
source('~/Desktop/Bike-Problem/enet_attempt.R', echo=TRUE)
train_train[,-c(13,14)]
test <- read.delim("~/Desktop/Bike-Problem/filtered_test.txt")
test$hour <- factor(test$hour)
test$year <- factor(test$year)
test$season <- factor(test$season)
test$holiday <- factor(test$holiday)
test$workingday <- factor(test$workingday)
test$weather <- factor(test$weather)
train <- read.delim("~/Desktop/Bike-Problem/filtered_train.txt")
train$hour <- factor(train$hour)
train$year <- factor(train$year)
train$season <- factor(train$season)
train$holiday <- factor(train$holiday)
train$workingday <- factor(train$workingday)
train$weather <- factor(train$weather)
library(gbm)
library(caret)
library(doParallel)
library(randomForest)
cl <-makeCluster(4)
registerDoParallel(cl)
#hold out .25 to estimate kaggle score
train_idx <-createDataPartition(train$casual,p=.75,list=F)
train_train <-train[train_idx,]
train_train$casual <-log(train_train$casual+1)
train_train$registered <-log(train_train$registered+1)
#trying out logs
train_test <- train[-train_idx,]
train_test_casual <- train_test$casual
train_test_registered <- train_test$registered
train_test <- train_test[,-c(13,14)]
control <- trainControl(method="LGOCV")
enet_casual <- train(as.matrix(train_train[,-c(13,14)]),as.matrix(train_train[,13]),
method = "enet",
trControl = control)
library(MASS)
source('~/Desktop/Bike-Problem/enet_attempt.R', echo=TRUE)
m_ridge_casual <- train(casual_formula,
data=train_test,
method="ridge",
preProc=c("center","scale"))
m_ridge_casual <- train(casual_formula,
data=train_train,
method="ridge",
preProc=c("center","scale"))
warnings()
train_train_casual <-train_train[,13]
train_train_registered <-train_train[,14]
train_train<-train_train[,-c(13,14)]
trans <- preProcess(train_train,method=c("BoxCox","center","scale","pca"))
test <- read.delim("~/Desktop/Bike-Problem/filtered_test.txt")
# test$hour <- factor(test$hour)
# test$year <- factor(test$year)
# test$season <- factor(test$season)
# test$holiday <- factor(test$holiday)
# test$workingday <- factor(test$workingday)
# test$weather <- factor(test$weather)
train <- read.delim("~/Desktop/Bike-Problem/filtered_train.txt")
# train$hour <- factor(train$hour)
# train$year <- factor(train$year)
# train$season <- factor(train$season)
# train$holiday <- factor(train$holiday)
# train$workingday <- factor(train$workingday)
# train$weather <- factor(train$weather)
library(gbm)
library(caret)
library(doParallel)
library(randomForest)
library(MASS)
cl <-makeCluster(4)
registerDoParallel(cl)
casual_formula <-as.formula(paste0("casual~",paste(colnames(train[-c(13,14)]),collapse="+")))
registered_formula <- as.formula(paste0("registered~",paste(colnames(train[-c(13,14)]),collapse="+")))
#hold out .25 to estimate kaggle score
train_idx <-createDataPartition(train$casual,p=.75,list=F)
train_train <-train[train_idx,]
train_train$casual <-log(train_train$casual+1)
train_train$registered <-log(train_train$registered+1)
#trying out logs
train_test <- train[-train_idx,]
train_test_casual <- train_test$casual
train_test_registered <- train_test$registered
train_test <- train_test[,-c(13,14)]
train_train_casual <-train_train[,13]
train_train_registered <-train_train[,14]
train_train<-train_train[,-c(13,14)]
trans <- preProcess(train_train,method=c("BoxCox","center","scale","pca"))
train$weekday<-numeric(train$weekday)
train$weekday<-numeric(train$weekday)
source('~/Desktop/Bike-Problem/averaged_model_jkg.R', echo=TRUE)
source('~/Desktop/Bike-Problem/averaged_model_jkg.R', echo=TRUE)
actual_total <- train_test_casual + train_test_registered
#
rmsle <- ((1/length(train_test_registered))*sum((log(predicted_total+1)-log(actual_total+1))**2))**.5
test <- read.delim("~/Desktop/Bike-Problem/filtered_test.txt")
test$hour <- factor(test$hour)
test$year <- factor(test$year)
test$season <- factor(test$season)
test$holiday <- factor(test$holiday)
test$workingday <- factor(test$workingday)
test$weather <- factor(test$weather)
train <- read.delim("~/Desktop/Bike-Problem/filtered_train.txt")
train$hour <- factor(train$hour)
train$year <- factor(train$year)
train$season <- factor(train$season)
train$holiday <- factor(train$holiday)
train$workingday <- factor(train$workingday)
train$weather <- factor(train$weather)
library(gbm)
library(caret)
library(doParallel)
library(randomForest)
train$casual <-log(train$casual+1)
train$registered <-log(train$registered+1)
# #hold out .25 to estimate kaggle score
# train_idx <-createDataPartition(train$casual,p=.70,list=F)
# train_train <-train[train_idx,]
# train_train$casual <-log(train_train$casual+1)
#
# train_train$registered <-log(train_train$registered+1)
# #trying out logs
#
# train_test <- train[-train_idx,]
# train_test_casual <- train_test$casual
# train_test_registered <- train_test$registered
# train_test <- train_test[,-c(13,14)]
casual_formula <-as.formula(paste0("casual~",paste(colnames(train[-c(13,14)]),collapse="+")))
registered_formula <- as.formula(paste0("registered~",paste(colnames(train[-c(13,14)]),collapse="+")))
##lets try to average em
gbm_casual<- gbm(casual_formula,n.trees=3000,data=train,
distribution="gaussian",interaction.depth=12,
train.fraction=.8,cv.folds=10)
source('~/Desktop/Bike-Problem/averaged_model_jkg.R', echo=TRUE)
gbm_casual$train.error
